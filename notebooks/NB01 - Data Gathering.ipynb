{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering\n",
    "\n",
    "**Name: Smyan Kapoor**\n",
    "\n",
    "**Candidate Number: 36745**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: system-ui; color: #000000; padding: 20px 30px 20px 20px; background-color: #FFFFFF; border-left: 8px solid #2B7A78; border-radius: 8px; box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1); max-width:700px\">\n",
    "\n",
    "**Notebook Overview:** In this stage, we fetch and structure Reddit data through the API in a secure and reproducible way, setting the foundation for subsequent analysis.\n",
    "\n",
    "#### Methodology\n",
    "\n",
    "- **Subreddit Selection:** We will target the Victoria Secret subreddit (r/VictoriasSecret) to answer the question: *How has Victoria Secret subreddit activity volume and engagement changed over time?*\n",
    "- **API Access:** Use the `requests` library to access Reddit's API with proper authentication, ensuring all API credentials are securely hidden from version control.\n",
    "- **Data Extraction:**\n",
    "  - Retrieve submission and comment data via authenticated requests.\n",
    "  - Extract and normalize JSON structures into clean tabular format using `pandas` techniques.\n",
    "- **Database Integration:**\n",
    "  - Dump the processed data into an **SQLite database**, mapping each dataframe to its corresponding table.\n",
    "  - Ensure proper **relational mapping** between the tables for efficient querying and integrity.\n",
    "- **Output:** The structured and linked data will be ready for access in downstream notebooks for analysis, visualization, and sentiment analysis.\n",
    "\n",
    "- **This pipeline ensures authenticated, secure, and well-structured data collection, ready for deeper exploration.**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ‚öôÔ∏è Importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smyankapoor/Desktop/code/vsco/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# this is stored in a utils.py within the notebooks folder\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit Selection  \n",
    "\n",
    "   We will focus on r/VictoriasSecret to analyze how community activity volume and sentiment have evolved over time. This subreddit provides insights into customer discussions, product reviews, and brand sentiment, making it ideal for tracking trends in both engagement volume and community perception.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Securing Credentials\n",
    "\n",
    "Using the `python-dotenv` library and a ```.env``` file in the root directory, into which we add our credentials, specifically \n",
    "\n",
    "```plaintext\n",
    "REDDIT_USERNAME=your_username\n",
    "REDDIT_PASSWORD=your_password\n",
    "REDDIT_CLIENT_ID=your_client_id\n",
    "REDDIT_CLIENT_SECRET=your_client_secret\n",
    "```\n",
    "\n",
    "we can then load our credentials safely using the `python-dotenv` library's load function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the .env file from the root directory\n",
    "load_dotenv('/Users/smyankapoor/Desktop/code/vsco/.env', override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The credentials are now stored in the `os.environ` dictionary, a safe place closer to the Operating System. We can use `os.getenv()` to retrieve the values from the dictionary when passing to the Reddit API without ever looking at them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's send a first request to the API to get an access token. We will pass this string in the headers of all subsequent requests to confirm our identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_auth = requests.auth.HTTPBasicAuth(os.getenv(\"CLIENT_ID\"), os.getenv(\"CLIENT_SECRET\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also send via [HTTP's](https://www.w3schools.com/tags/ref_httpmethods.asp) POST method, our Reddit username and password and identify ourselves using a `User-Agent` header as per Reddit documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data = {\"grant_type\": \"password\", \"username\": os.getenv('REDDIT_USERNAME'), \"password\": os.getenv('REDDIT_PASSWORD')}\n",
    "headers = {\"User-Agent\": f\"LSE DS105W (2024/25) Data Collection by {os.getenv('REDDIT_USERNAME')}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we send the request using a function that returns the reddit access token :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "Response: {'access_token': 'eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJ1c2VyIiwiZXhwIjoxNzcwMDc2NDk4LjkyNjU2MSwiaWF0IjoxNzY5OTkwMDk4LjkyNjU2MSwianRpIjoiamY0NkRJNnlkTlJGOXFacmJvRWNERm1aR3pGNlB3IiwiY2lkIjoic3IyV0kwamdRbG9CdjJfZFJaVjVnQSIsImxpZCI6InQyXzFhMm9sNDk3dmsiLCJhaWQiOiJ0Ml8xYTJvbDQ5N3ZrIiwiYXQiOjEsImxjYSI6MTcyNzk1OTAxNTU2NSwic2NwIjoiZUp5S1Z0SlNpZ1VFQUFEX193TnpBU2MiLCJmbG8iOjl9.k3tpqXjT7T5D4FNQLg5sLLry_RQRXf3pgsrXZb_aT7OWJwtfIN6jf9tjsod5iFDaEheUSctDp5Xy2lFRe__KcKIdhkkV-JtfQ4mbJtkpSu_Vh--3ufNUELrQjzGqU6IG5UhLkviNQbBtLjrj77duH_6Bh2V93KLSk-vV4GadOBeiZC17RFjIpI0fSCMTNWEwIX8ja8bQ5WEedACbY2-6SUDpM40gfR27DYS9F4iZCWAIaHcJIFwfST1tqgousEkq-j9XgGwm29koTt5thZdsdlU3HqyDEO8T2K7m6fhf7ab5UT5YTin5fhwiAnIfRwU-RTNchAy5QSMQ-_LJYNZhtA', 'token_type': 'bearer', 'expires_in': 86400, 'scope': '*'}\n"
     ]
    }
   ],
   "source": [
    "ACCESS_TOKEN_ENDPOINT = \"https://www.reddit.com/api/v1/access_token\"\n",
    "response = requests.post(ACCESS_TOKEN_ENDPOINT, auth=client_auth, data=post_data, headers=headers)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Response: {response.json()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "my_token = get_reddit_access_token(client_auth, post_data, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From now on, all requests need to be followed by these HTTP HEADERS\n",
    "headers = {\"Authorization\": f\"bearer {my_token}\", \"User-Agent\": f\"LSE DS105W (2024/25) API practice by {os.getenv('REDDIT_USERNAME')}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "Extracting data from the Reddit API and turning it into json:\n",
    "\n",
    "+ Using the headers and token we got from the previous step, we can now make authenticated requests to the Reddit API\n",
    "\n",
    "+ We use ```requests.get()``` to fetch the data from the API in three seperate functions which return lists, first for subreddit metadata, next for posts and finally for comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>name</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t5_2t5r0</td>\n",
       "      <td>VictoriasSecret</td>\n",
       "      <td>25581</td>\n",
       "      <td>1.322596e+09</td>\n",
       "      <td>‚ú®Welcome to r/VictoriasSecret! ‚ú® An unofficial...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit_id             name  subscribers   created_utc  \\\n",
       "0     t5_2t5r0  VictoriasSecret        25581  1.322596e+09   \n",
       "\n",
       "                                         description  \n",
       "0  ‚ú®Welcome to r/VictoriasSecret! ‚ú® An unofficial...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise our list of subs\n",
    "subs_list = ['VictoriasSecret']\n",
    "\n",
    "sub_data = fetch_subreddit_info(subs_list, headers)\n",
    "unfiltered_sub_df = pd.DataFrame(sub_data)\n",
    "\n",
    "rename_dict = {'name': 'subreddit_id', 'display_name': 'name', \"public_description\": 'description'}\n",
    "\n",
    "filtered_sub_df = unfiltered_sub_df[[\"name\", \"display_name\", \"subscribers\", \"created_utc\", \"public_description\"]].rename(columns=rename_dict)\n",
    "\n",
    "filtered_sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function fetch_recent_posts takes in a list of subreddits and retrieves recent post data using Reddit‚Äôs API. It accepts days and max_pages as arguments. Using the time library, it converts the days parameter into a Unix timestamp to filter for posts created within that timeframe.\n",
    "\n",
    "For each subreddit, the outer loop iterates through the list of subreddits, while the inner loop handles pagination. Since Reddit‚Äôs API allows a maximum of 100 posts per request (with a cap of 1000 per subreddit), we page through results using the after parameter, which holds the unique ID of the last post from the previous batch. This ensures continuity across pages.\n",
    "\n",
    "Posts are sorted by new to prioritize the most recent content, allowing us to capture fresh data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posts fetched: 881 | Last sub accessed: r/VictoriasSecret\n"
     ]
    }
   ],
   "source": [
    "# get our post data \n",
    "posts_list = fetch_recent_posts(subs_list, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the functions above, we find that we get numbers below 1000, for each subreddit despite that being the call limit. However, research reveals that this is because deleted posts are counted in the 1000 post limit, but are not called on. Hence, as long as a post has been deleted within the timeframe we are calling, it will reduce the number of 'live' posts we can request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>saved</th>\n",
       "      <th>mod_reason_title</th>\n",
       "      <th>gilded</th>\n",
       "      <th>clicked</th>\n",
       "      <th>title</th>\n",
       "      <th>link_flair_richtext</th>\n",
       "      <th>...</th>\n",
       "      <th>media_metadata.pqgal6cpjn3g1.s.u</th>\n",
       "      <th>media_metadata.pqgal6cpjn3g1.id</th>\n",
       "      <th>media_metadata.qjqfr4cpjn3g1.status</th>\n",
       "      <th>media_metadata.qjqfr4cpjn3g1.e</th>\n",
       "      <th>media_metadata.qjqfr4cpjn3g1.m</th>\n",
       "      <th>media_metadata.qjqfr4cpjn3g1.p</th>\n",
       "      <th>media_metadata.qjqfr4cpjn3g1.s.y</th>\n",
       "      <th>media_metadata.qjqfr4cpjn3g1.s.x</th>\n",
       "      <th>media_metadata.qjqfr4cpjn3g1.s.u</th>\n",
       "      <th>media_metadata.qjqfr4cpjn3g1.id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>VictoriasSecret</td>\n",
       "      <td>Anyone know if HR reports at the company are a...</td>\n",
       "      <td>t2_1owb8fkprp</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Anonymous?</td>\n",
       "      <td>[{'e': 'text', 't': 'Employee Talk'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>VictoriasSecret</td>\n",
       "      <td>majority of their apparel is $25-35 sale, but ...</td>\n",
       "      <td>t2_6pbjd6qz</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>I‚Äôm so disappointed I thought I could get an e...</td>\n",
       "      <td>[{'e': 'text', 't': 'Sales &amp;amp; Deals'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>VictoriasSecret</td>\n",
       "      <td>Girlfriend found this bra off pinterest saying...</td>\n",
       "      <td>t2_cz61plgx</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Need help finding bra for girlfriend!!</td>\n",
       "      <td>[{'e': 'text', 't': 'Item ID'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>VictoriasSecret</td>\n",
       "      <td>Hi! Has anyone tried this slip dress that is v...</td>\n",
       "      <td>t2_bhcnmkak</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Is this too long for short people? VS Maxi Sli...</td>\n",
       "      <td>[{'e': 'text', 't': 'Fit Check'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>VictoriasSecret</td>\n",
       "      <td>Item screenshot in comments below \\*üëá\\n\\nI wor...</td>\n",
       "      <td>t2_c4cpxh7h</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Issue with item- be careful!</td>\n",
       "      <td>[{'e': 'text', 't': 'Apparel'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>None</td>\n",
       "      <td>VictoriasSecret</td>\n",
       "      <td>Does anyone know what old items will be return...</td>\n",
       "      <td>t2_161rv8</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Old LSF x Pink Items?</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>None</td>\n",
       "      <td>VictoriasSecret</td>\n",
       "      <td>been planning my haul for bf for about 2 weeks...</td>\n",
       "      <td>t2_dtf9b0gtu</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>my black friday haul! üõíüéÄ</td>\n",
       "      <td>[{'e': 'text', 't': 'üí∏ Black Friday'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>None</td>\n",
       "      <td>VictoriasSecret</td>\n",
       "      <td></td>\n",
       "      <td>t2_w2pgx5mb</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Is the ‚Äúplan your buy‚Äù working for anyone else...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>None</td>\n",
       "      <td>VictoriasSecret</td>\n",
       "      <td>Ughhh I love it so much and its marked down to...</td>\n",
       "      <td>t2_ui9864u1</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Does anyone know if this will be back in stock?</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>https://preview.redd.it/pqgal6cpjn3g1.jpg?widt...</td>\n",
       "      <td>pqgal6cpjn3g1</td>\n",
       "      <td>valid</td>\n",
       "      <td>Image</td>\n",
       "      <td>image/jpg</td>\n",
       "      <td>[{'y': 186, 'x': 108, 'u': 'https://preview.re...</td>\n",
       "      <td>1867.0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>https://preview.redd.it/qjqfr4cpjn3g1.jpg?widt...</td>\n",
       "      <td>qjqfr4cpjn3g1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>None</td>\n",
       "      <td>VictoriasSecret</td>\n",
       "      <td></td>\n",
       "      <td>t2_ugoxmrg3k</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Can someone help me find this? Is it online? I...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>881 rows √ó 6389 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    approved_at_utc        subreddit  \\\n",
       "0              None  VictoriasSecret   \n",
       "1              None  VictoriasSecret   \n",
       "2              None  VictoriasSecret   \n",
       "3              None  VictoriasSecret   \n",
       "4              None  VictoriasSecret   \n",
       "..              ...              ...   \n",
       "876            None  VictoriasSecret   \n",
       "877            None  VictoriasSecret   \n",
       "878            None  VictoriasSecret   \n",
       "879            None  VictoriasSecret   \n",
       "880            None  VictoriasSecret   \n",
       "\n",
       "                                              selftext author_fullname  saved  \\\n",
       "0    Anyone know if HR reports at the company are a...   t2_1owb8fkprp  False   \n",
       "1    majority of their apparel is $25-35 sale, but ...     t2_6pbjd6qz  False   \n",
       "2    Girlfriend found this bra off pinterest saying...     t2_cz61plgx  False   \n",
       "3    Hi! Has anyone tried this slip dress that is v...     t2_bhcnmkak  False   \n",
       "4    Item screenshot in comments below \\*üëá\\n\\nI wor...     t2_c4cpxh7h  False   \n",
       "..                                                 ...             ...    ...   \n",
       "876  Does anyone know what old items will be return...       t2_161rv8  False   \n",
       "877  been planning my haul for bf for about 2 weeks...    t2_dtf9b0gtu  False   \n",
       "878                                                        t2_w2pgx5mb  False   \n",
       "879  Ughhh I love it so much and its marked down to...     t2_ui9864u1  False   \n",
       "880                                                       t2_ugoxmrg3k  False   \n",
       "\n",
       "    mod_reason_title  gilded  clicked  \\\n",
       "0               None       0    False   \n",
       "1               None       0    False   \n",
       "2               None       0    False   \n",
       "3               None       0    False   \n",
       "4               None       0    False   \n",
       "..               ...     ...      ...   \n",
       "876             None       0    False   \n",
       "877             None       0    False   \n",
       "878             None       0    False   \n",
       "879             None       0    False   \n",
       "880             None       0    False   \n",
       "\n",
       "                                                 title  \\\n",
       "0                                           Anonymous?   \n",
       "1    I‚Äôm so disappointed I thought I could get an e...   \n",
       "2               Need help finding bra for girlfriend!!   \n",
       "3    Is this too long for short people? VS Maxi Sli...   \n",
       "4                         Issue with item- be careful!   \n",
       "..                                                 ...   \n",
       "876                              Old LSF x Pink Items?   \n",
       "877                           my black friday haul! üõíüéÄ   \n",
       "878  Is the ‚Äúplan your buy‚Äù working for anyone else...   \n",
       "879    Does anyone know if this will be back in stock?   \n",
       "880  Can someone help me find this? Is it online? I...   \n",
       "\n",
       "                           link_flair_richtext  ...  \\\n",
       "0        [{'e': 'text', 't': 'Employee Talk'}]  ...   \n",
       "1    [{'e': 'text', 't': 'Sales &amp; Deals'}]  ...   \n",
       "2              [{'e': 'text', 't': 'Item ID'}]  ...   \n",
       "3            [{'e': 'text', 't': 'Fit Check'}]  ...   \n",
       "4              [{'e': 'text', 't': 'Apparel'}]  ...   \n",
       "..                                         ...  ...   \n",
       "876                                         []  ...   \n",
       "877     [{'e': 'text', 't': 'üí∏ Black Friday'}]  ...   \n",
       "878                                         []  ...   \n",
       "879                                         []  ...   \n",
       "880                                         []  ...   \n",
       "\n",
       "                      media_metadata.pqgal6cpjn3g1.s.u  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "876                                                NaN   \n",
       "877                                                NaN   \n",
       "878                                                NaN   \n",
       "879  https://preview.redd.it/pqgal6cpjn3g1.jpg?widt...   \n",
       "880                                                NaN   \n",
       "\n",
       "     media_metadata.pqgal6cpjn3g1.id  media_metadata.qjqfr4cpjn3g1.status  \\\n",
       "0                                NaN                                  NaN   \n",
       "1                                NaN                                  NaN   \n",
       "2                                NaN                                  NaN   \n",
       "3                                NaN                                  NaN   \n",
       "4                                NaN                                  NaN   \n",
       "..                               ...                                  ...   \n",
       "876                              NaN                                  NaN   \n",
       "877                              NaN                                  NaN   \n",
       "878                              NaN                                  NaN   \n",
       "879                    pqgal6cpjn3g1                                valid   \n",
       "880                              NaN                                  NaN   \n",
       "\n",
       "    media_metadata.qjqfr4cpjn3g1.e  media_metadata.qjqfr4cpjn3g1.m  \\\n",
       "0                              NaN                             NaN   \n",
       "1                              NaN                             NaN   \n",
       "2                              NaN                             NaN   \n",
       "3                              NaN                             NaN   \n",
       "4                              NaN                             NaN   \n",
       "..                             ...                             ...   \n",
       "876                            NaN                             NaN   \n",
       "877                            NaN                             NaN   \n",
       "878                            NaN                             NaN   \n",
       "879                          Image                       image/jpg   \n",
       "880                            NaN                             NaN   \n",
       "\n",
       "                        media_metadata.qjqfr4cpjn3g1.p  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "876                                                NaN   \n",
       "877                                                NaN   \n",
       "878                                                NaN   \n",
       "879  [{'y': 186, 'x': 108, 'u': 'https://preview.re...   \n",
       "880                                                NaN   \n",
       "\n",
       "    media_metadata.qjqfr4cpjn3g1.s.y  media_metadata.qjqfr4cpjn3g1.s.x  \\\n",
       "0                                NaN                               NaN   \n",
       "1                                NaN                               NaN   \n",
       "2                                NaN                               NaN   \n",
       "3                                NaN                               NaN   \n",
       "4                                NaN                               NaN   \n",
       "..                               ...                               ...   \n",
       "876                              NaN                               NaN   \n",
       "877                              NaN                               NaN   \n",
       "878                              NaN                               NaN   \n",
       "879                           1867.0                            1080.0   \n",
       "880                              NaN                               NaN   \n",
       "\n",
       "                      media_metadata.qjqfr4cpjn3g1.s.u  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "876                                                NaN   \n",
       "877                                                NaN   \n",
       "878                                                NaN   \n",
       "879  https://preview.redd.it/qjqfr4cpjn3g1.jpg?widt...   \n",
       "880                                                NaN   \n",
       "\n",
       "     media_metadata.qjqfr4cpjn3g1.id  \n",
       "0                                NaN  \n",
       "1                                NaN  \n",
       "2                                NaN  \n",
       "3                                NaN  \n",
       "4                                NaN  \n",
       "..                               ...  \n",
       "876                              NaN  \n",
       "877                              NaN  \n",
       "878                              NaN  \n",
       "879                    qjqfr4cpjn3g1  \n",
       "880                              NaN  \n",
       "\n",
       "[881 rows x 6389 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalised_post_df = pd.json_normalize(posts_list)\n",
    "\n",
    "normalised_post_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets now filter this into a dataframe with only the relevant columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we include subreddit since we need it for the fetch later\n",
    "final_post_data_df = normalised_post_df[['id','subreddit_id','title', 'author', 'created_utc', 'score', 'upvote_ratio', 'num_comments', 'ups', 'subreddit', 'name']]\n",
    "\n",
    "final_post_data_df = final_post_data_df.rename(columns={'name': 'post_id'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```fetch_comments_from_post``` function below has similar functionality to the ```fetch_recent_posts``` function, using the ```after``` parameter for pagination. However, it takes in a row of a data frame instead of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the fetch comments function to each row in the DataFrame taking headers as an input\n",
    "\n",
    "initial_comments =  final_post_data_df.apply(fetch_comments_from_post, headers=headers, axis=1);\n",
    "\n",
    "\n",
    "# flatten the list of lists returned\n",
    "\n",
    "some_comments = [comment for comments in initial_comments for comment in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62472"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(some_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some_comments[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspecting the list of dictionaries ```sum_comments```, we find that after going into the ```replies``` key of the orginal comment, and consequently going into ```data``` and then ```children``` we can access replies to a root comment. This process repeats for each comment. The function ```extract_comments``` below uses recursion, a processes that I learnt about in the Harvard CS50 online course. Every time it comes across a reply within a comment, it calls itself, taking that reply as an argument and looking for replies within it until there are no more left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_comments = [comment for comment in extract_comments(some_comments)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "      <th>ups</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ml1y867</td>\n",
       "      <td>t3_1jpttjo</td>\n",
       "      <td>t2_4gezv939</td>\n",
       "      <td>1.743614e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>Tariffs can always be reversed</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_1jpttjo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ml1wzpj</td>\n",
       "      <td>t3_1jpttjo</td>\n",
       "      <td>t2_k9rq6iij0</td>\n",
       "      <td>1.743614e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>Already priced in. We won‚Äôt know the full econ...</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_1jpttjo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ml1xmqv</td>\n",
       "      <td>t3_1jpttjo</td>\n",
       "      <td>t2_11sc4n</td>\n",
       "      <td>1.743614e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>&amp;gt; Already priced in\\n\\n&amp;gt; We won‚Äôt know t...</td>\n",
       "      <td>1</td>\n",
       "      <td>t1_ml1wzpj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ml1xd06</td>\n",
       "      <td>t3_1jpttjo</td>\n",
       "      <td>t2_2snajx0o</td>\n",
       "      <td>1.743614e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>Wait till we get a partial Treasury default on...</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_1jpttjo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ml1y9qw</td>\n",
       "      <td>t3_1jpttjo</td>\n",
       "      <td>t2_4r22p8do</td>\n",
       "      <td>1.743614e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>I remember when reddit would vehemently fight ...</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_1jpttjo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151402</th>\n",
       "      <td>mfu688r</td>\n",
       "      <td>t3_1j17uuc</td>\n",
       "      <td>t2_dvy2u</td>\n",
       "      <td>1.741033e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>Fidelity and vanguard also route orders to mar...</td>\n",
       "      <td>1</td>\n",
       "      <td>t1_mflq7zs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151403</th>\n",
       "      <td>mfh4z1c</td>\n",
       "      <td>t3_1j16jd4</td>\n",
       "      <td>t2_4tx3lxsw</td>\n",
       "      <td>1.740855e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>It's a tiny company that markets random home g...</td>\n",
       "      <td>5</td>\n",
       "      <td>t3_1j16jd4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151404</th>\n",
       "      <td>mfh5er6</td>\n",
       "      <td>t3_1j16jd4</td>\n",
       "      <td>t2_1hpcjzd4db</td>\n",
       "      <td>1.740856e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>They have positive revenue and cash in hand</td>\n",
       "      <td>1</td>\n",
       "      <td>t1_mfh4z1c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151405</th>\n",
       "      <td>mfha55q</td>\n",
       "      <td>t3_1j16jd4</td>\n",
       "      <td>t2_4tx3lxsw</td>\n",
       "      <td>1.740857e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>All companies have positive revenue. What do y...</td>\n",
       "      <td>3</td>\n",
       "      <td>t1_mfh5er6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151406</th>\n",
       "      <td>mficzmt</td>\n",
       "      <td>t3_1j16jd4</td>\n",
       "      <td>t2_pnavzoewb</td>\n",
       "      <td>1.740869e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>Sorry but that is the squatty potty of these s...</td>\n",
       "      <td>2</td>\n",
       "      <td>t3_1j16jd4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151407 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       comment_id     post_id         author   created_utc  score  \\\n",
       "0         ml1y867  t3_1jpttjo    t2_4gezv939  1.743614e+09      1   \n",
       "1         ml1wzpj  t3_1jpttjo   t2_k9rq6iij0  1.743614e+09      1   \n",
       "2         ml1xmqv  t3_1jpttjo      t2_11sc4n  1.743614e+09      1   \n",
       "3         ml1xd06  t3_1jpttjo    t2_2snajx0o  1.743614e+09      1   \n",
       "4         ml1y9qw  t3_1jpttjo    t2_4r22p8do  1.743614e+09      1   \n",
       "...           ...         ...            ...           ...    ...   \n",
       "151402    mfu688r  t3_1j17uuc       t2_dvy2u  1.741033e+09      1   \n",
       "151403    mfh4z1c  t3_1j16jd4    t2_4tx3lxsw  1.740855e+09      5   \n",
       "151404    mfh5er6  t3_1j16jd4  t2_1hpcjzd4db  1.740856e+09      1   \n",
       "151405    mfha55q  t3_1j16jd4    t2_4tx3lxsw  1.740857e+09      3   \n",
       "151406    mficzmt  t3_1j16jd4   t2_pnavzoewb  1.740869e+09      2   \n",
       "\n",
       "                                                     body  ups   parent_id  \n",
       "0                          Tariffs can always be reversed    1  t3_1jpttjo  \n",
       "1       Already priced in. We won‚Äôt know the full econ...    1  t3_1jpttjo  \n",
       "2       &gt; Already priced in\\n\\n&gt; We won‚Äôt know t...    1  t1_ml1wzpj  \n",
       "3       Wait till we get a partial Treasury default on...    1  t3_1jpttjo  \n",
       "4       I remember when reddit would vehemently fight ...    1  t3_1jpttjo  \n",
       "...                                                   ...  ...         ...  \n",
       "151402  Fidelity and vanguard also route orders to mar...    1  t1_mflq7zs  \n",
       "151403  It's a tiny company that markets random home g...    5  t3_1j16jd4  \n",
       "151404        They have positive revenue and cash in hand    1  t1_mfh4z1c  \n",
       "151405  All companies have positive revenue. What do y...    3  t1_mfh5er6  \n",
       "151406  Sorry but that is the squatty potty of these s...    2  t3_1j16jd4  \n",
       "\n",
       "[151407 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_comments_df = pd.DataFrame(final_comments)\n",
    "final_comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumping Dataframes into SQLite Database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we will dump our dataframes into tables that match their current structure\n",
    "- they will be mapped according to the specifications outlined, using primary and secondary keys\n",
    "- Our database engine is set up in ```utils.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subreddits Table\n",
    "| Column Name    | Data Type    | Reasoning                                                                                                                                               |\n",
    "|----------------|--------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| subreddit_id   | CHAR(8)      | characters are fixed at 8 for the subreddit ID, to ensure consistent length and efficient storage for the ID.          |\n",
    "| name           | VARCHAR(20)  | a variable length string for the subreddit name capped at 20, sufficient for most subreddit names.                 |\n",
    "| subscribers    | INTEGER      | the number of subscribers will always be a whole number              |\n",
    "| created_utc    | INTEGER      | time in UTC is an integer            |\n",
    "| description    | VARCHAR(200) | A variable length string for the subreddit‚Äôs description capped at 200            |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits_text = \"\"\"\n",
    "    subreddit_id CHAR(8) PRIMARY KEY,\n",
    "    name VARCHAR(20),\n",
    "    subscribers INTEGER,\n",
    "    created_utc INTEGER,\n",
    "    description VARCHAR(200)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posts Table\n",
    "| Column Name    | Data Type    | Reasoning                                                                                                 |\n",
    "|----------------|--------------|-----------------------------------------------------------------------------------------------------------|\n",
    "| post_id        | CHAR(10)     | Unique post ID, fixed-length for efficient lookups.                                                      |\n",
    "| subreddit_id   | CHAR(8)      | ID linking post to its subreddit.                                                                        |\n",
    "| title          | VARCHAR(600) | Post title, capped for consistency and storage efficiency.                                               |\n",
    "| author         | VARCHAR(50)  | Reddit username or ID of the author.                                                                     |\n",
    "| created_utc    | INTEGER      | Unix timestamp for sorting and filtering.                                                                |\n",
    "| score          | INTEGER      | Net score (upvotes ‚àí downvotes).                                                                         |\n",
    "| upvote_ratio   | FLOAT        | Proportion of upvotes to total votes.                                                                    |\n",
    "| num_comments   | INTEGER      | Number of comments on the post.                                                                          |\n",
    "| ups            | INTEGER      | Raw upvotes (not net score).                                                                             |\n",
    "| subreddit      | VARCHAR(20)  | Subreddit name, stored for quick access without needing a join.                                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_text = \"\"\"\n",
    "    post_id CHAR(10) PRIMARY KEY,\n",
    "    subreddit_id CHAR(8),\n",
    "    title VARCHAR(600),\n",
    "    author VARCHAR(50),\n",
    "    created_utc INTEGER,\n",
    "    score INTEGER,\n",
    "    upvote_ratio FLOAT,\n",
    "    num_comments INTEGER,\n",
    "    ups INTEGER,\n",
    "    subreddit VARCHAR(20),\n",
    "    FOREIGN KEY (subreddit_id) REFERENCES subreddits(subreddit_id)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments Table\n",
    "| Column Name    | Data Type    | Reasoning                                                                                                                                               |\n",
    "|----------------|--------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| comment_id     | CHAR(6)      | fixed-length string for the comment ID, ensuring consistency and efficient lookups.       |\n",
    "| post_id        | CHAR(10)     | fixed-length string for the post ID, linking the comment to its post.        |\n",
    "| author         | VARCHAR(20)  | variable-length string for the comment's author, capped at 20 for typical username lengths.     |\n",
    "| created_utc    | INTEGER      | timestamp stored as an integer in UTC (Unix epoch time) for sorting and filtering.                    |\n",
    "| score          | INTEGER      | integer to store the comment's score (upvotes minus downvotes), representing a whole number.                                      |\n",
    "| body           | VARCHAR(40000)| variable-length string to store the comment body, capped at 40,000 for larger comments.   |\n",
    "| ups            | INTEGER      | integer to store the number of upvotes, a simple whole number useful for ranking comments.                  |\n",
    "| parent_id      | CHAR(10)     | fixed-length string to store the parent comment ID (if any), for comment threads. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_text = \"\"\"\n",
    "    comment_id CHAR(6) PRIMARY KEY,\n",
    "    post_id CHAR(10),  \n",
    "    author VARCHAR(20),\n",
    "    created_utc INTEGER,\n",
    "    score INTEGER,\n",
    "    body VARCHAR(40000),\n",
    "    ups INTEGER,\n",
    "    parent_id CHAR(10),\n",
    "    FOREIGN KEY (post_id) REFERENCES posts(post_id)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use create_table, a function outlined in ```utils.py``` which takes in two strings as arguments, one as a table title, and another as the SQL code to create the table as per our requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table('subreddits', subreddits_text)\n",
    "create_table('posts', posts_text)\n",
    "create_table('comments', comments_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use method='multi' since it sends multiple rows per query instead of one row at a time, however, this increases RAM usage for large data sets, so we we do it groups of 100 using the chunksize argument as a good midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151407"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_comments_df.to_sql('comments', engine, if_exists='append', index=False, chunksize=100, method='multi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3472"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_post_data_df.drop(columns='id').to_sql('posts', engine, if_exists='append', index=False, chunksize=100, method='multi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sub_df.to_sql('subreddits', engine, if_exists='append', index=False, chunksize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "\n",
    "+ We will access the ```database.db``` file in the exploratory data analysis notebook to continue the investigation. We will begin by querying the database to understand the data. Next we will reshape and analyse the data to gain some insights into our research question. The analysis will be supported by visualizations to help interpret and present the results effectively. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
